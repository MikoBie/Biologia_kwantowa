---
title: "Markov nowy algorytm"
author: "Mikolaj Biesaga"
date: ''
output:
  html_notebook: default
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(
    echo = FALSE, warning = FALSE, message = FALSE, include = FALSE,fig.width = 7, fig.height = 5, fig.align = "center"
)
pdf.options(encoding = "ISOLatin2")
```

```{r loading_libraries}
library(gtools)
#library(plyr)
library(knitr)
library(readr)
library(kableExtra)
#library(DiagrammeR)
library(tidyverse)
```
```{r loading_data}
#przykłady z ludźmi

example_1 <- read_csv("~/Desktop/Psychologia kwantowa/Survey/los_chem_zera_3_3_10_57_34.csv")
example_2 <- read_csv("~/Desktop/Psychologia kwantowa/Survey/los_chem_zera_3_1_14_17_18.csv")

#przygotowanie dla nowego algorytmu
example_1_new <- example_1$key
example_2_new <- example_2$key

#przygotowanie dla starego algorytmu
example_1_old <- example_1
colnames(example_1_old)[2] <- "gap_bin"
example_2_old <- example_2
colnames(example_2_old)[2] <- "gap_bin"


#symulacje Michała

setwd("~/Desktop/Biologia_kwantowa")
vector_of_files<-list.files("~/Desktop/Biologia_kwantowa",pattern = ".dat$")

#with heteregonity
with_heteregonity <- read.delim(paste0(vector_of_files[1]),header = F, sep = '\t')
#without heteregonity
without_heteregonity <- read.delim(paste0(vector_of_files[2]),header = F, sep = '\t')

# Mój pierwszy ruch to policzenie odstępów.

with_heteregonity$gap <- c(NA,c(with_heteregonity$V1[2:length(with_heteregonity$V1)])-c(with_heteregonity$V1[1:(length(with_heteregonity$V1)-1)]))
without_heteregonity$gap <- c(NA,c(without_heteregonity$V1[2:length(without_heteregonity$V1)])-c(without_heteregonity$V1[1:(length(without_heteregonity$V1)-1)]))

# Pierwszy czas to NA bo czas pierwszego spike'a jest od włączenia nagrywania, dlatego bez żalu go usuwam

with_heteregonity_chart <- with_heteregonity[-1,]
without_heteregonity_chart <- without_heteregonity[-1,]

#zbinowanie wyników

with_heteregonity_chart$gap_bin <- as.numeric(as.character(cut(with_heteregonity_chart$gap,c(0, seq(10, 120, by = 10)),c(1:12))))
without_heteregonity_chart$gap_bin <- as.numeric(as.character(cut(without_heteregonity_chart$gap,c(0, seq(10, 200, by = 10)),c(1:20))))

#przygotowanie dla nowego algorytmu
with_heteregonity_new <- with_heteregonity_chart$gap_bin
without_heteregonity_new <- without_heteregonity_chart$gap_bin


```
##

Tak jak rozmawialiśmy w Ann Arbor Ciebie interesowałoby pokazanie stabilności jednego ciągu strzałów neuronów względem drugiego. Pomysł, o którym wtedy rozmawialiśmy opierał się na tym, żeby wziąć odstępy pomiędzy strzałami, zbinować je jakoś i korzystając z metody opartej na ukrytych ciągach Markova porównać je między sobą. Strona techniczna tej meotdy opisana jest na przykładzie poniżej, tutaj chciałbym tylko pokazać mój sposób myślenia. 

Pomysł:
1) Zbinować czasy pomiędzy strzałami neuronów w obu ciągach.
2) Przy pomocy metody opartej na ukrytych ciągach Markowa postarać się jak najlepiej przewidzieć każdy z ciągów. Jak najlepiej w tym przypadku oznacza, że wygenerowany przez algorytm ciąg będzie jak najbardziej podobny do oryginalnego.
3) Sprawdzenie na jakiej podstawie ten algorytm wygenerował najlepszy ciąg. W idealnym świecie w przypadku jednego ciągu byłoby to np. w przypadku jednego ciągu zawsze na podstawie poprzedniego wyrazu, a w przypadku drugiego byłoby to w 25% na podstawie poprzedniego wyrazu, 25% na podstawie dwóch poprzedniech wyrazów, 25% na podstawie trzech poprzednich wyrazów i 25% na podstawie czterech poprzednich wyrazów. Innymi słowy porównanie ze sobą na jakiej podstawie najlepsze przewidywania zostały zrobione.
3*) Można też porównać ze sobą wyniki dopasowania ciągów wygenerowanych przez algorytm i oryginalnych. Ma to jednak tę wadę, że jest mocno zależne od liczby binów, która powinna być taka sama.

## Stary algorytm

Chcąc przewidzieć ostatni element takiego ciągu <img src="ciag.pdf" alt="some text"> przy klasycznym podejściu policzyłbym po prostu frekwencje jedynek (7) i podzieliłbym na liczbę wszystkich elementów (14). Nic ciekawego by z tego nie wyniknęło bo prawdopodobieństwa zer i jedynek byłyby dalej takie same. Dlatego zamiast zakładać, że pojawienie się w tym ciagu zera albo jedynki jest niezależne od tego jaki był poprzedni wyraz założę, że poprzedni wyraz ma znaczenie (dalej będę to nazywał uwzględnieniem historii jeden). W związku z tym tworzona jest tabela, w której zapisuje wystąpienia tym razem par (patrz poniżej Tabela 1.).

```{r tabela1, include=TRUE}

h1 <- c(0,0,1,1)
q <- c(0,1,0,1)
n <- c(2,5,4,2)

data_frame(h1,q,n) %>%
  kable("html",caption="Tabela 1. Macierz dla historii jeden") %>%
  kable_styling("bordered", full_width = FALSE, position="center") %>%
  column_spec(c(1:3), width="5.5em")
```

Widać z niej, że skoro ostatnim elementem przed tym, który chcę zgadnąć jest jedynka to z prawdopodobieństwem $\frac{2}{3}$ kolejnym będzie zero. Jest to wyższe prawdopodobieństwo niż to uzyskane przy założeniu, że zdarzenia są niezależne.

Algorytm, którego używam porównuje między sobą prawdopodobieństwa uzyskane przy założeniu niezależności oraz historii od jednego do pięciu. Wybiera najwyższe i na jego podstawie przewiduje kolejny element. Jest to algorytm uczący się, bo przy każdym kolejnym elemencie ta tabela się aktualizuje. Innymi słowy na podstawie takiej tabeli jak ta poniżej wybierane jest najwyższe prawdopodobieństwo, a co za tym idzie kolejny element ciągu (akurat ta tabela jest dla historii jeden i dwa, ale dla dłuższych historii będzie wyglądać analogicznie).

```{r tabela2, include=TRUE}
h2 <- c("-","-",0,0) 
h1 <- c(1,1,1,1)
q <- c(0,1,0,1)
n <- c(4,2,2,2)
p <- c(2/3,1/3,1/2,1/2) %>% round(2)

data_frame(h2,h1,q,n,p) %>%
  kable("html",caption="Tabela 2. Macierz na podstawie historii jeden i dwa") %>%
  kable_styling("bordered", full_width = FALSE, position="center") %>%
  column_spec(c(1:5), width="5.5em")
```

## Nowy algorytm

Nowy algorytm jeśli chodzi o założenia jest bliźniaczo podobny do starego. Różnica pojawia się w momencie, w którym porównywane są ze sobą prawdopodobieństwa, a więc wybierana jest odpowiednia historia na podstawie, której dokonywane są przewidywania. Poprzednio dla każdej historii tworzona była oddzielna tabela prawdopodobieństwa. Wybierane były dwa wiersze i porównywane były prawdopodbieństwa między nimi. Wiersz z największym prawdopodobieństwem wybierany był jako ten na podstawie, którego przewidywany był kolejny element ciągu. Teraz ten proces wygląda inaczej ponieważ dla  każdej dłuższej historii uwzględniane są też wszystkie krótsze. Innymi słowy zamiast tworzyć jedną macierz dla każdej historii odzielnie tworzona jest jedna macierz dla wszystkich historii. Poniżej jest przykład dla historii dwa dla tego samego ciągu co w przypadku starego algorytmu <img src="ciag.pdf" alt="some text">.

Wiedząc, że będę uwzględniał historię do dwóch tworzę następującą tabelę. W kolumnach "1" i "0" zapisywane są częstości wystąpienia ciągów `h2,h1,1` oraz `h2,h1,0`. W pierwszym kroku zapisywane są frekwencje dla historii jeden (porównaj Tabela 3). Na jej podstawie można policzyć, że prawdopodobieństwo wystąpienia zera po jedynce wynosi $p(0)=\frac{4+4}{2+4+2+4}=\frac{2}{3}$, czyli dokładnie tyle samo co w przypadku starego algorytmu. W kojenym kroku do tej tabeli dodawane są frekwencje dla historii dwa (porównaj Tabela 4). Z niej wynika, że dla historii jeden prawdopodobieństwo wystąpienia zera po jedynce wynosi $p(0)=\frac{4+2+4+2}{2+0+4+2+2+2+4+2}=\frac{1}{2}$, a dla historii dwa $p(0)=\frac{4+2}{4+2+2+2}=\frac{6}{10}$.

<div class="col2">
```{r tabela3, include=TRUE}
knitr::include_graphics("tabela4_2.pdf")
h2 <- c(0,0,1,1) 
h1 <- c(0,1,0,1)
jeden <- c(5,2,5,2)
zero <- c(2,4,2,4)

data_frame(h2,h1,jeden,zero) %>%
  rename("0"=zero,
         "1"=jeden) %>%
  kable("html",caption="Tabela 3. Macierz częstości dla historii jeden",output=FALSE) %>%
  kable_styling("bordered", full_width = FALSE, position="center") %>%
  column_spec(c(1:4), width="6em") %>%
  column_spec(1,italic=TRUE)

h2 <- c(0,0,1,1) 
h1 <- c(0,1,0,1)
jeden <- c("5+2","2+2","5+2","2+0")
zero <- c("2+0","4+2","2+2","4+2")
data_frame(h2,h1,jeden,zero) %>%
  rename("0"=zero,
         "1"=jeden) %>%
  kable("html",caption="Tabela 4. Macierz częstości dla historii jeden i dwa",output=FALSE) %>%
  kable_styling("bordered", full_width = FALSE, position="center") %>%
  column_spec(c(1:4), width="6em")

```
</div>

## Wnioski z obu algorytmów

Ogólnie dzięki temu, że trochę podłubałem w tych algorytmach to zorientowałem się, że zwiększenie historii czasem powoduje, że będziemy gorzej przewidywać, a nie lepiej, tzn. tak jak mówiłem na samym początku ten algorytm, który my wykorzystujemy do przewidywania ludzi (dzięki temu, że trochę teraz podłubałem w tym to napisałem lepszy - lepiej przewidujący ludzi) brał całkowicie arbitralnie historię 1, 3 i 5. Dzięki tak dobranej historii następujący ciąg był w stanie przewidzieć z prawdopodobieństwem 0.701342, jednak biorąc po uwagę tylko historię jeden prawdopodobieństwo rośnie do 0.7147651.  
`r (example_1_new)`   


Ogólnie poprzedni akapit miał pokazać, że dodanie historii niekoniecznie zaowocuje zwiększeniem przewidywalności. U ludzi to jeszcze można wytłumaczyć tym, że heurystyka losowości obejmuje tylko historię jeden. Co patrząc w kontekście literatury ma sens, ale to akurat w tym wypadku bez znaczenia.

## Dane z symulacji

Po przydługim wstępie, wreszcie czas na to co zrobiłem nowego. Po pierwsze na sam początek puściłem wszystko jeszcze raz za pomocą nowego algorytmu (na razie głównie będę się skupiał na przewidywalności jako mierze dobroci dopasowania. Do rozkładu historii przejdę na sam koniec). Poniżej jest tabela, w której są prawdopodobieństwa przewidzenia ciągu.

Użyta metoda|Wykorzystana historia |With heteregonity | Without heteregonity
------------|----------------------|------------------|---------------------
Stary algorytm|1,2,3,4,5|0.352518|0.151898
Nowy algorytm|1,2,3,4,5|0.45|0.1125
Nowy algorytm|1|0.4071429|0.0875
Nowy algorytm|2|0.3357143|0.1
Nowy algorytm|3|0.2571429|0.0875
Nowy algorytm|4|0.2071429|0.0625
Nowy algorytm|5|0.1714286|0.0875
Nowy algorytm|1,2|0.4357143|0.125
Nowy algorytm|1,3|0.4071429|**0.1625**
Nowy algorytm|1,4|0.3928571|0.0625
Nowy algorytm|1,5|0.3857143|0.0875
Nowy algorytm|1,2,3|0.45|0.15
Nowy algorytm|1,2,4|0.45|0.0875
Nowy algorytm|1,2,5|0.414285|0.1
Nowy algorytm|1,3,4|0.4|0.15
Nowy algorytm|1,3,5|0.3857143|**0.1625**
Nowy algorytm|1,4,5|0.3928571|0.1
Nowy algorytm|1,2,3,4|**0.4642857**|0.1125
Nowy algorytm|1,3,4,5|0.3857143|**0.1625**
Nowy algorytm|1,2,3,5|0.4428571|0.1375
Nowy algorytm|1,2,4,5|0.45|0.1

Jedno co na pewno widać na podstawie powyższej tabeli, że zgodnie z oczekiwaniami długość historii wpływa na przewidywanie, tzn. w przypadku ciągu z heteregonity dodanie historii 5 obniża prawdopodobieństwo przewidzenia ciągu. Dzieje się tak ponieważ algorytmowi wydaje się, że znalazł wzór przy historii 5, a tak naprawdę był to przypadek. W przypadku without heteregonity widać, że tak naprawdę do przewidzenia jest potrzebna historia 1 i 3. Historia 4 i 5 nie pomaga ale też nie przeszkadza, za to historia 2 już przeszkadza. Oczywiście powyżej przedstawiona tabela nie zawiera wszystkich możliwych kombinacji bo projektując mój algorytm nie wpadłem na to, że to będzie miało takie znaczenie. Myślałem, że najwyższe prawdopodobieństwo wskażę najlepszy z punktu widzenia globalnego wyraz. Okazało się to nieprawdą więc historię musiałem wymuszać niejako ręcznie. Wybrałem najciekawsze według mnie kombinacje (zawsze uwzględniałem historię jeden bo nie uwzględnienie jej zwiększa element losowy). Dużą wadą ogólnie tych metod jest to, że zawsze będzie w nich jakiś element losowy, tzn. zawsze znajdzie się wyraz (przynajmniej jeden), który będzie trzeba wylosować bo prawdopodobieństwo wielu binów będzie takie samo. Dlatego powyższe wartości mogą się wahać +/- 0.02, raczej nie więcej bo góra dwa, trzy wyrazy były losowane. Oczywiście rozwiązaniem jest policzenie wartości oczekiwanej na podstawie powiedzmy 100 czy 1000 prób, ale to mimo tego, że ten algorytm jest całkiem szybki to trochę to potrwa. Poza tym w tym momencie taka dokładność nie byłą mi potrzebna zwłaszcza przy nierównej liczbie binów.

## Ujednolicenie liczby binów

Jedna uwaga na wstępie zanim przejdę do szczegółów. Z dłuższego ciągu, czyli z with heteregonity brałem tylko pierwsze 80 wyrazów bo długość ciągu też ma znaczenie.

###1. Decyle
Miałem zacząć od odchyleń standardowych ale łatwiej było od decyli. W ten sposób chciałem uzyskać 10 przedziałów, w których będzie po równo wyników (w sensie procentowym). Dopiero po policzeniu tego zorientowałem się, że był to chybiony pomysł, tzn. pomysł był chybiony nie dlatego, że nic z tego nie wyszło tylko z założenia był chybiony. Jakoś nie wpadłem na to, że przecież tak naprawdę w binowaniu chodzi nam o zbinowanie podobnych czasów, a nie części rozkładu. Poniżej tabela z wynikami ale tak jak można się było spodziewać nic specjalnie nie wnosi.

Użyta metoda|Wykorzystana historia |With heteregonity | Without heteregonity
------------|----------------------|------------------|---------------------
Nowy algorytm|1,2,3,4,5|0.0875|0.0875
Nowy algorytm|1|0.05|0.075
Nowy algorytm|2|0.0875|0.0875
Nowy algorytm|3|0.125|0.05
Nowy algorytm|4|0.1125|0.075
Nowy algorytm|5|0.125|0.0625
Nowy algorytm|1,2|0.05|0.0875
Nowy algorytm|1,3|0.0625|0.075
Nowy algorytm|1,4|0.05|0.05
Nowy algorytm|1,5|0.0375|0.1125
Nowy algorytm|1,2,3|0.075|0.1
Nowy algorytm|1,2,4|0.05|0.0875
Nowy algorytm|1,2,5|0.075|0.1
Nowy algorytm|1,3,4|0.025|0.0625
Nowy algorytm|1,3,5|0.05|0.0875
Nowy algorytm|1,4,5|0.0625|0.075
Nowy algorytm|1,2,3,4|0.0875|0.0875
Nowy algorytm|1,3,4,5|0.025|0.0375
Nowy algorytm|1,2,3,5|0.0875|0.0875
Nowy algorytm|1,2,4,5|0.05|0.075

###2. Odchylenie standardowe
Z odchyleniem standardowym główny problem jest taki, że jest nierówna liczba binów. Dla with heteregonity jest ich 7, a dla without heteregonity jest 6. Przy nierównej liczbie binów powinno być łatwiej przewidzieć ten ciąg z mniejszą liczbą binów, a wcale tak nie jest. Nie wprost pokazuje to, że with heteregonity jest bardziej stabilny.

Użyta metoda|Wykorzystana historia |With heteregonity | Without heteregonity
------------|----------------------|------------------|---------------------
Nowy algorytm|1,2,3,4,5|0.5625|0.4
Nowy algorytm|1|0.55|0.425
Nowy algorytm|2|0.4625|0.35
Nowy algorytm|3|0.3625|0.3125
Nowy algorytm|4|0.2625|0.3
Nowy algorytm|5|0.2125|0.275
Nowy algorytm|1,2|0.575|0.4375
Nowy algorytm|1,3|0.5625|**0.4625**
Nowy algorytm|1,4|0.5625|0.4125
Nowy algorytm|1,5|0.575|0.3875
Nowy algorytm|1,2,3|0.5625|0.4
Nowy algorytm|1,2,4|0.5625|0.4
Nowy algorytm|1,2,5|**0.575**|0.3875
Nowy algorytm|1,3,4|**0.575**|0.4125
Nowy algorytm|1,3,5|**0.575**|0.4125
Nowy algorytm|1,4,5|**0.575**|0.4
Nowy algorytm|1,2,3,4,5|0.5625|0.4
Nowy algorytm|1,3,4,5|**0.575**|0.4
Nowy algorytm|1,2,3,5|**0.575**|0.4125
Nowy algorytm|1,2,4,5|0.5625|0.3875

Ten sam problem co poprzednio. Żeby wyciągać wnioski na temat historii trzeba policzyć wartość oczekiwaną.

###3. Wymuszenie 10 przedziałów
Wydaje mi się, że najbardziej naturalnym sposobem zbinowania w tym przypadku będzie podzielenie po prostu czasu na równą ilość przedziałów. W tabeli poniżej przedstawione są wyniki dla 10 równych binów. W przypadku with heteregonity bin jest szerokości 10.8 sekund, a without heteregonity 17.72 sekund.

Użyta metoda|Wykorzystana historia |With heteregonity | Without heteregonity
------------|----------------------|------------------|---------------------
Nowy algorytm|1,2,3,4,5|0.5375|0.25
Nowy algorytm|1|0.4375|0.25
Nowy algorytm|2|0.4375|0.2
Nowy algorytm|3|0.375|0.175
Nowy algorytm|4|0.175|0.075
Nowy algorytm|5|0.225|0.125
Nowy algorytm|1,2|0.5|**0.2625**
Nowy algorytm|1,3|0.5|**0.2625**
Nowy algorytm|1,4|0.475|0.2125
Nowy algorytm|1,5|0.5|**0.2625**
Nowy algorytm|1,2,3|**0.55**|0.2375
Nowy algorytm|1,2,4|0.525|0.2375
Nowy algorytm|1,2,5|**0.55**|0.25
Nowy algorytm|1,3,4|0.5|0.25
Nowy algorytm|1,3,5|0.5125|0.2375
Nowy algorytm|1,4,5|0.5125|0.2125
Nowy algorytm|1,2,3,4|0.5375|0.225
Nowy algorytm|1,3,4,5|0.5125|**0.2625**
Nowy algorytm|1,2,3,5|**0.55**|0.2125
Nowy algorytm|1,2,4,5|0.5375|0.225

Tak jak pisałem poprzednio muszę policzyć wartość oczekiwaną tych prawdopodobieństw przy policzeniu tego przynajmniej 100 razy wtedy będzie można wnioskować o historii lepiej.

## Wnioski

Ogólnie mam wrażenie, że robi się coraz bardziej ciekawiej, tzn. nowy algorytm w połączeniu z ostatnim zbinowaniem robi chyba to co miał robić - pokazuje większą stabilność with heteregonity w porównaniu z without heteregonity. Żeby można było powiedzieć coś więcej o historii trzeba będzie policzyć to więcej razy. Może przez weekend ustawie drugi komputer do liczenia tego. Mam jeszcze dwa pomysły na to jak ulepszyć ten algorytm (tzn. jeszcze nie wiem czy to w czymś pomoże ale jeszcze bym w tym podłubał).
Ciekawi mnie teraz jak ten algorytm będzie się zachowywał jeśli wysłałbyś mi dane bardziej do siebie podobne, tzn. z tego co zrozumiałem to te, które teraz od Ciebie dostałem są dość skrajne i oczywiste w rozróżnieniu.
```{r echo=F}
# sum(list_with_heteregonity_full[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_full[[2]]==without_heteregonity_new)/80
# 
# sum(list_with_heteregonity_only_h1[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h1[[2]]==without_heteregonity_new)/80
# 
# sum(list_with_heteregonity_only_h2[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h2[[2]]==without_heteregonity_new)/80
# 
# sum(list_with_heteregonity_only_h3[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h3[[2]]==without_heteregonity_new)/80
# 
# sum(list_with_heteregonity_only_h4[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h4[[2]]==without_heteregonity_new)/80
# 
# sum(list_with_heteregonity_only_h5[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h5[[2]]==without_heteregonity_new)/80
# 
# sum(list_with_heteregonity_only_h12[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h12[[2]]==without_heteregonity_new)/80
# 
# sum(list_with_heteregonity_only_h13[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h13[[2]]==without_heteregonity_new)/80
# 
# sum(list_with_heteregonity_only_h14[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h14[[2]]==without_heteregonity_new)/80
# 
# sum(list_with_heteregonity_only_h15[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h15[[2]]==without_heteregonity_new)/80
# 
# sum(list_with_heteregonity_only_h123[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h123[[2]]==without_heteregonity_new)/80
# 
# sum(list_with_heteregonity_only_h124[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h124[[2]]==without_heteregonity_new)/80
# 
# sum(list_with_heteregonity_only_h125[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h125[[2]]==without_heteregonity_new)/80
# 
# sum(list_with_heteregonity_only_h134[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h134[[2]]==without_heteregonity_new)/80
# 
# sum(list_with_heteregonity_only_h135[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h135[[2]]==without_heteregonity_new)/80
# 
# sum(list_with_heteregonity_only_h145[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h145[[2]]==without_heteregonity_new)/80
# 
# 
# sum(list_with_heteregonity_only_h11234[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h1234[[2]]==without_heteregonity_new)/80
# 
# sum(list_with_heteregonity_only_h1345[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h1345[[2]]==without_heteregonity_new)/80
# 
# sum(list_with_heteregonity_only_h1235[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h1235[[2]]==without_heteregonity_new)/80
# 
# sum(list_with_heteregonity_only_h1245[[2]][1:80]==with_heteregonity_new[1:80])/80
# sum(list_without_heteregonity_only_h1245[[2]]==without_heteregonity_new)/80
```

<!-- CSS styling -->
<style>
    html {
        height: 100%;
        font-size: 62.5%;
    }
    body {
        height: 100%;
        font-size: 1.6em;
        text-align: justify;
        font-family: "Trebuchet MS", "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", sans-serif;
    }
    h1, h2, h3 {
        text-align: center;
    }
    h4.author, h4.date {
        margin: 0.75em 0 0 0;
        text-align: center;
    }
    h2, h3, h4, h5, h6 {
        margin: 2em 0 1em 0;
    }
    div#header {
        margin: 1em 0 1em 0;
    }
    hr {
        margin: 2em 0 2em 0;
    }
    pre {
        margin-bottom: 2em;
    }
</style>

<style>
  .col2 {
    columns: 2 200px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 200px; /* chrome, safari */
    -moz-columns: 2 200px;    /* firefox */
  }
  .col3 {
    columns: 3 100px;
    -webkit-columns: 3 100px;
    -moz-columns: 3 100px;
  }
</style>


